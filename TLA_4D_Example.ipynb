{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "TLA_4D_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkqw9U4gaxAv"
      },
      "source": [
        "#hide\n",
        "#skip\n",
        "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW3PeajoV3RD"
      },
      "source": [
        "In this notebook, we will run a ready-made network starting from some ATLAS data, which is already normalized. There is also an alternative to train the network from scratch.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulUIQK3V3RK"
      },
      "source": [
        "## Look into the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5gooOPV3RK"
      },
      "source": [
        "First we need to make sure that Python 3.8 is used in the notebook. It is required in order to open this certain .pkl-file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Caw3L3kVV3RL"
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDW7fnoV3RM"
      },
      "source": [
        "We take a pickle dataset, and open into Pandas (after importing pandas). Note that you have to change the paths to the directory where your processed files are. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRp_In9LV3RM"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Change these paths to point to where you have stored the datasets. \n",
        "data_path = '/content/monojet_Zp2000.0_DM_50.0_chan3.csv'\n",
        "preprocessed_path = '/content/preprocessed.csv'\n",
        "\n",
        "# Data is separated by ; for event details\n",
        "# Specific event object information is separated by ,\n",
        "# This means no specific length limit for the rows\n",
        "# From https://stackoverflow.com/questions/55188544/pandas-how-to-workaround-error-tokenizing-data\n",
        "data = pd.read_csv(data_path, sep='\\n', header=None)[0].str.split(';', expand=True)\n",
        "\n",
        "# The final state objects, as described in Table 1, are stored in a one-line-per-event text (csv) file,\n",
        "# where each line has variable length and contains 3 event-specifiers, followed by the kinematic fea-\n",
        "# tures for each object in the event. The format of CSV files is:\n",
        "# event ID; process ID; event weight; MET; METphi; obj1, E1, pt1, eta1, phi1; obj2, E2, pt2, eta2, phi2; . . .\n",
        "\n",
        "# TLDR: Strip out the unnecessary fields, not a 1 to 1 compression but data compression of relevant objects\n",
        "# Your task is to compress the \"four-momentum\" of the jet particles that are described as follows in the .csv file:\n",
        "# obj1, E1, pt1, eta1, phi1\n",
        "# You should select only one kind of particle - the jets (id = \"j\") - and compress their four-momentum using a network \n",
        "# that is similar (or the same) to the one in this Jupyter notebook:\n",
        "\n",
        "# Strip out the eventID, process ID, event weight, MET, METphi\n",
        "data_columns = data.columns\n",
        "relevant_data = data[range(5, len(data_columns))]\n",
        "\n",
        "# Not quite ideal but we can reconvert to CSV to take advantage of some features\n",
        "relevant_data.to_csv(preprocessed_path, sep=';', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hGcmV4iYbb7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "columns = [\"type\",\"E\",\"pt\",\"eta\",\"phi\"] # m is missing \n",
        "\n",
        "# Reads the data through Pandas\n",
        "train = pd.read_csv(preprocessed_path, sep=',', lineterminator=';', names=columns)\n",
        "test = pd.read_csv(preprocessed_path, sep=',', lineterminator=';', names=columns)\n",
        "\n",
        "# We only want to compress a single type of object\n",
        "train = train[train[\"type\"] == 'j'][columns[1:]]\n",
        "test = test[test[\"type\"] == 'j'][columns[1:]]\n",
        "\n",
        "# Normalize data as expected\n",
        "def normalize_data(df):\n",
        "  df[\"eta\"] /= 5\n",
        "  df[\"phi\"] /= 3\n",
        "  df[\"E\"] = np.log(df[\"E\"]) / np.log(10)\n",
        "  df[\"pt\"] = np.log(df[\"pt\"]) / np.log(10)\n",
        "  return df\n",
        "\n",
        "train = normalize_data(train)\n",
        "test = normalize_data(test)\n",
        "\n",
        "# To get an idea of the order of magnitude we are going to see in the plots we show the first elements \n",
        "# in the samples:\n",
        "print('Training sample:')\n",
        "print(train.head())\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Testing sample:')\n",
        "print(test.head())\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('The number of entries in the training data:', len(train))\n",
        "print('The number of entries in the validation data:', len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XYCmchuV3RN"
      },
      "source": [
        "Now we plot the data using the matplotlib library. The units reflect the normalization, but it's the shape that we care about. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq3gkWNQV3RN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unit_list = ['[log(GeV)]', '[rad/3]', '[rad/3]', '[log(GeV)]']\n",
        "variable_list = [r'$E$', r'$p_T$', r'$\\eta$', r'$\\phi$']\n",
        "\n",
        "branches=[\"E\",\"pt\",\"eta\",\"phi\"] # No m\n",
        "\n",
        "n_bins = 100\n",
        "\n",
        "for kk in range(0,len(branches)):\n",
        "    n_hist_data, bin_edges, _ = plt.hist(train[branches[kk]], color='gray', label='Input', alpha=1, bins=n_bins)\n",
        "    plt.xlabel(xlabel=variable_list[kk] + ' ' + unit_list[kk])\n",
        "    plt.ylabel('# of events')\n",
        "    plt.savefig(\"fourmomentum_\"+branches[kk],dpi=300)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evx3BbWIV3RO"
      },
      "source": [
        "## Setting up the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8--tvbC5V3RO"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdV8VE_aV3RO"
      },
      "source": [
        "Adding the two datasets as TensorDatasets to PyTorch (also loading all other classes we'll need later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzuVW9IQV3RP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from fastai import learner\n",
        "from fastai.data import core\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cpu'\n",
        "\n",
        "# m seems to some clutch thing going on\n",
        "train_x = train\n",
        "test_x = test\n",
        "train_y = train_x  # y = x since we are building an autoencoder\n",
        "test_y = test_x\n",
        "\n",
        "# Constructs a tensor object of the data and wraps them in a TensorDataset object.\n",
        "train_ds = TensorDataset(torch.tensor(train_x.values, dtype=torch.float).to(device), torch.tensor(train_y.values, dtype=torch.float).to(device))\n",
        "valid_ds = TensorDataset(torch.tensor(test_x.values, dtype=torch.float).to(device), torch.tensor(test_y.values, dtype=torch.float).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4nlEeqyV3RP"
      },
      "source": [
        "We now set things up to load the data, and we use a batch size that was optimized by previous students...note also that this is fastai v2, migration thanks to Jessica Lastow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW1qekLAV3RP"
      },
      "source": [
        "bs = 256\n",
        "\n",
        "# Converts the TensorDataset into a DataLoader object and combines into one DataLoaders object (a basic wrapper\n",
        "# around several DataLoader objects). \n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
        "dls = core.DataLoaders(train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8n4j7YjV3RQ"
      },
      "source": [
        "### Preparing the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4NUQYnNV3RQ"
      },
      "source": [
        "Here we have an example network. Details aren't too important, as long as they match what was already trained for us...in this case we have a LeakyReLU, tanh activation function, and a number of layers that goes from 4 to 200 to 20 to 3 (number of features in the hidden layer that we pick for testing compression) and then back all the way to 4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOZk4u-V3RQ"
      },
      "source": [
        "class AE_3D_200(nn.Module):\n",
        "  def __init__(self, input_features, output_features):\n",
        "    super(AE_3D_200, self).__init__()\n",
        "    # Encoder network\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(input_features, 200),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(200, 200),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(200, 20),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(20, output_features),\n",
        "    )\n",
        "    # Decoder network\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(output_features, 20),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(20, 200),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(200, 200),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(200, input_features),\n",
        "    )\n",
        "\n",
        "  def encode(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  def decode(self, x):\n",
        "    return self.decoder(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.decode(self.encode(x))\n",
        "\n",
        "  def describe(self):\n",
        "    return 'AE-200-20'\n",
        "\n",
        "model = AE_3D_200(4, 3)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IyKq5zGV3RR"
      },
      "source": [
        "We now have to pick a loss function - MSE loss is appropriate for a compression autoencoder since it reflects the [(input-output)/input] physical quantity that we want to minimize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaYYo86RV3RR"
      },
      "source": [
        "from fastai.metrics import mse\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#bn_wd = False  # Don't use weight decay for batchnorm layers\n",
        "#true_wd = True  # weight decay will be used for all optimizers\n",
        "wd = 1e-6\n",
        "\n",
        "recorder = learner.Recorder()\n",
        "learn = learner.Learner(dls, model=model, wd=wd, loss_func=loss_func, cbs=recorder)\n",
        "# was learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, wd=wd, callback_fns=ActivationStats, bn_wd=bn_wd, true_wd=true_wd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNodZl2eV3RS"
      },
      "source": [
        "## Alternative 1: Running a pre-trained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxyvRT5vV3RS"
      },
      "source": [
        "Now we load the pre-trained network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA-vCGitV3RS"
      },
      "source": [
        "# learn.load(\"4D_TLA_leading\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqwSx68wV3RS"
      },
      "source": [
        "Then we evaluate the MSE on this network - it should be of the order of 0.001 or less if all has gone well...if it has not trained as well (note the pesky 0-mass peak above...) then it's going to be a bit higher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVrbD6m0V3RT"
      },
      "source": [
        "# learn.validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRtpDJ6VV3RU"
      },
      "source": [
        "## Alternative 2: Training a new network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9rLWOF4V3RU"
      },
      "source": [
        "Instead of using a pre-trained network, an alternative is to train a new network and use that instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JepyuZkUV3RU"
      },
      "source": [
        "First, we want to find the best learning rate. The learning rate is a hyper-paramater that sets how much the weights of the network will change each step with respect to the loss gradient.\n",
        "\n",
        "Then we plot the loss versus the learning rates. We're interested in finding a good order of magnitude of learning rate, so we plot with a log scale.\n",
        "\n",
        "A good value for the learning rates is then either:\n",
        "- one tenth of the minimum before the divergence\n",
        "- when the slope is the steepest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK62ZcmAV3RU"
      },
      "source": [
        "from fastai.callback import schedule\n",
        "\n",
        "lr_min, lr_steep = learn.lr_find()\n",
        "\n",
        "print('Learning rate with the minimum loss:', lr_min)\n",
        "print('Learning rate with the steepest gradient:', lr_steep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--PWytt7V3RV"
      },
      "source": [
        "Now we want to run the training!\n",
        "\n",
        "User-chosen variables:\n",
        "- n_epoch: The number of epochs, i.e how many times the to run through all of the training data once (i.e the 1266046 entries, see cell 2)\n",
        "- lr: The learning rate. Either choose lr_min, lr_steep from above or set your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZljHgChV3RV"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.perf_counter() # Starts timer\n",
        "# learn.fit_one_cycle(n_epoch=100, lr=lr_min)\n",
        "learn.fit_one_cycle(100, slice(lr_steep)) # guess this changed\n",
        "end = time.perf_counter() # Ends timer\n",
        "delta_t = end - start\n",
        "print('Training took', delta_t, 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvm-M6SV3RV"
      },
      "source": [
        "Then we plot the loss as a function of batches and epochs to check if we reach a plateau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87A8wjGIV3RW"
      },
      "source": [
        "recorder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgaWOgw3V3RY"
      },
      "source": [
        "Then we evaluate the MSE on this network - it should be of the order of 0.001 or less if all has gone well...if it has not trained as well (note the pesky 0-mass peak above...) then it's going to be a bit higher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Us6teXDV3RZ"
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CofT-oiPV3RZ"
      },
      "source": [
        "Let's plot all of this, with ratios (thanks to code by Erik Wallin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCXXhlW7V3RZ"
      },
      "source": [
        "## Plotting the outputs of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AX-BQXV3RZ"
      },
      "source": [
        "Lazy-save of our output files (they'll also be on screen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY4yAM43V3RZ"
      },
      "source": [
        "import os\n",
        "save_dir = \"plotOutput\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44wgpkMqV3Ra"
      },
      "source": [
        "A function in case we want to un-normalize and get back to physical quantities..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx9pktRiV3Ra"
      },
      "source": [
        "def custom_unnormalize(df):\n",
        "    df['eta'] = df['eta'] * 5\n",
        "    df['phi'] = df['phi'] * 3\n",
        "    df['E'] = 10**df['E']\n",
        "    # df['m'] = 10**df['m']\n",
        "    df['pt'] = 10**(df['pt'])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1znOGD4V3Ra"
      },
      "source": [
        "Make the histograms from the dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEhugMS_V3Ra"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "plt.close('all')\n",
        "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
        "variable_list = [r'$E$', r'$p_T$', r'$\\eta$', r'$\\phi$']\n",
        "line_style = ['--', '-']\n",
        "colors = ['orange', 'c']\n",
        "markers = ['*', 's']\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "save = True # Option to save figure\n",
        "\n",
        "# Histograms\n",
        "idxs = (0, 100000)  # Choose events to compare\n",
        "data = torch.tensor(test[idxs[0]:idxs[1]].values, dtype=torch.float).to(device)\n",
        "# data = torch.tensor(test[idxs[0]:idxs[1]].values, dtype=torch.float).double().to(device)\n",
        "pred = model(data)\n",
        "pred = pred.detach().to('cpu').numpy()\n",
        "data = data.detach().to('cpu').numpy()\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=test.columns)\n",
        "pred_df = pd.DataFrame(pred, columns=test.columns)\n",
        "\n",
        "# Unnormalizing creates inf values...\n",
        "unnormalized_data_df = custom_unnormalize(data_df)\n",
        "unnormalized_pred_df = custom_unnormalize(pred_df)    \n",
        "    \n",
        "alph = 0.8\n",
        "n_bins = 200\n",
        "for kk in np.arange(4):\n",
        "    plt.figure()\n",
        "    n_hist_data, bin_edges, _ = plt.hist(data[:, kk], color=colors[1], label='Input', alpha=1, bins=n_bins)\n",
        "    n_hist_pred, _, _ = plt.hist(pred[:, kk], color=colors[0], label='Output', alpha=alph, bins=bin_edges)\n",
        "    plt.suptitle(test.columns[kk])\n",
        "    plt.xlabel(test.columns[kk])\n",
        "    plt.ylabel('Number of events')\n",
        "    # ms.sciy()\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    if save:\n",
        "        plt.savefig(os.path.join(save_dir,test.columns[kk]+'.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVTywCb9V3Rb"
      },
      "source": [
        "def getRatio(bin1,bin2):\n",
        "    bins = []\n",
        "    for b1,b2 in zip(bin1,bin2):\n",
        "        if b1==0 and b2==0:\n",
        "            bins.append(0.)\n",
        "        elif b2==0:\n",
        "            bins.append(None)\n",
        "        else:\n",
        "            bins.append((float(b2)-float(b1))/b1)\n",
        "    return bins   \n",
        "\n",
        "rat = getRatio(n_hist_data,n_hist_pred)\n",
        "\n",
        "sum = 0.0\n",
        "for i in rat:\n",
        "  sum = sum + i\n",
        "\n",
        "print(sum / len(rat))\n",
        "print(rat)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}